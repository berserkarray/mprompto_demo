import json
import os
import asyncio
import requests
import logging
import time
from openai import OpenAI
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

client = OpenAI(
    api_key = os.environ.get("OPENAI_API_KEY")
)

TARGET_API_URL = os.environ.get("TARGET_API_URL", "https://playground.mprompto.com:3000/api/v1/demo/clients/load-json-data")


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s'
)

def generate_bulk_questions(raw_text, question_prompt, num_questions=20, model="gpt-4o"):

    bulk_prompt = (
        f"Using the following context, generate exactly {num_questions} unique, concise, and use-caseâ€“driven questions. "
        "Each question must be a single sentence that starts with a capital letter and ends with a question mark. "
        "Return the questions as a JSON array of strings (do not include any extra text).\n\n"
        f"Context:\n{raw_text}"
    )
    
    messages = [
        {"role": "system", "content": question_prompt},
        {"role": "user", "content": bulk_prompt}
    ]
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=1000,
            temperature=0.2
        )
        raw_output = response.choices[0].message.content.strip()
        logging.info(f"Raw output: {raw_output}")
        if raw_output.startswith("```json"):
            raw_output = raw_output[len("```json"):].strip()
        if raw_output.endswith("```"):
            raw_output = raw_output[:-3].strip()
        
        logging.info("Bulk questions generated. Parsing JSON...")
        questions = json.loads(raw_output)
        if not (isinstance(questions, list) and len(questions) == num_questions):
            raise ValueError("Parsed questions do not match the expected count.")
        return questions
    except Exception as e:
        logging.error(f"Error generating bulk questions: {e}")
        return None

def generate_answer_for_question(raw_text, question, answer_prompt, model="o1-mini"):
    
    prompt = (
        f"Using the following context, answer the question below.\n\n"
        f"Question:\n{question}\n\n"
        "Your answer must include\n"
        "1. containing a detailed, highly professional answer addressing the question.\n"
        "2. 'Reasoning:\n"
        "    - 'Facet considered:' followed by facets considered to answer the question (single line answer, short but informative),\n"
        "    - 'Pros considered:' followed by exactly THREE advantages (comma-separated),\n"
        "    - 'Cons considered:' followed by exactly TWO drawbacks (comma-separated).\n\n"
        f"Context:\n{raw_text}\n\n"
        "Generate your answer as plain text with the two sections and no extra commentary."
    )
    
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": answer_prompt}
    ]
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=600,
            temperature=0.3
        )
        answer_text = response.choices[0].message.content.strip()
        logging.info("Answer generated for question.")
        return answer_text
    except Exception as e:
        logging.error(f"Error generating answer for question '{question}': {e}")
        return None

def extract_answer_details(answer_text, question, model="o1-preview"):
    
    extraction_prompt = (
        '''You are an expert at extracting structured information. The following question was asked:
{question}
Below is an answer text generated by an LLM in response to this question. The answer text includes a brief summary of the question and relevant context data, along with a clearly and logically presented analysis, a balanced view explicitly stating the pros and cons, and a well-reasoned recommendation that aligns with the user's needs. 

Your job is to extract 4 things from it:
- The main response (which is a professional, concise, two-sentence response similar to what a veteran shop attendant might say).
- The facet (whatever multiple facets of the product was considered during decision-making, explain in a nice manner, in 1 sentence).
- The pros (exactly 3 points, present the pros beautifully, they are the key selling point, 1 sentence per point).
- The cons (exactly 2 points, present them beautifully, 1 sentence per point).

Return the result in exactly the following JSON format:
{{
 "answers": "<the main concise response>",
 "facet": ["<facets>"],
 "pros": ["<pro1>", "<pro2>", "<pro3>"],
 "cons": ["<con1>", "<con2>"]
}}
Do not include any additional commentary. Use only the text provided below.
Text: {answer_text}'''.format(question=question, answer_text=answer_text)
    )
    
    messages = [
        {"role": "system", "content": "You are a precise data extraction assistant."},
        {"role": "user", "content": extraction_prompt}
    ]
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.2
        )
        extracted_raw = response.choices[0].message.content.strip()
        logging.info(f"Extraction raw output: {extracted_raw}")
        
        if extracted_raw.startswith("```json"):
            extracted_raw = extracted_raw[len("```json"):].strip()
        if extracted_raw.endswith("```"):
            extracted_raw = extracted_raw[:-3].strip()
        
        if not extracted_raw:
            raise ValueError("Extraction output is empty after cleaning.")
        
        data = json.loads(extracted_raw)
        # Validate counts:
        if not (isinstance(data.get("facet"), list) and len(data.get("facet")) == 1):
            raise ValueError("Facet count error.")
        if not (isinstance(data.get("pros"), list) and len(data.get("pros")) == 3):
            raise ValueError("Pros count error.")
        if not (isinstance(data.get("cons"), list) and len(data.get("cons")) == 2):
            raise ValueError("Cons count error.")
        if not data.get("answers"):
            raise ValueError("Main answer is empty.")
        return data
    except Exception as e:
        logging.error(f"Error extracting answer details: {e}")
        return None

def generate_final_qna_container(container_id, raw_text, question_prompt, answer_prompt, num_pairs=20, model = 'gpt-4o'):
    
    final_container = {
        "id": container_id,
        "question_prompt": question_prompt,
        "answer_prompt": answer_prompt,
        "data": {
            "qa": []
        }
    }
    
    questions = generate_bulk_questions(raw_text, question_prompt, num_questions = 20, model = model)
    if not questions:
        logging.error("Failed to generate questions.")
        return None
    logging.info(f"Generated {len(questions)} questions. Proceeding with answer generation...")
    
    for idx, question in enumerate(questions, start=1):
        logging.info(f"Processing Q&A pair {idx}...")
        answer_text = generate_answer_for_question(raw_text, question, answer_prompt, model=model)
        if not answer_text:
            logging.error(f"Skipping Q&A pair {idx} due to answer generation failure.")
            continue
        
        extracted = extract_answer_details(answer_text, question, model)
        if not extracted:
            logging.error(f"Skipping Q&A pair {idx} due to extraction failure.")
            continue
        
        qa_pair = {
            "question": question,
            "answers": extracted.get("answers"),
            "facet": extracted.get("facet"),
            "pros": extracted.get("pros"),
            "cons": extracted.get("cons")
        }
        final_container["data"]["qa"].append(qa_pair)
        
        time.sleep(1)
    
    if len(final_container["data"]["qa"]) != num_pairs:
        logging.warning(f"Expected {num_pairs} QA pairs but only assembled {len(final_container['data']['qa'])}.")
    
    return json.dumps(final_container, indent=2)



# ---------------------------
# FastAPI Setup
# ---------------------------
app = FastAPI()

class QNARequest(BaseModel):
    id: str
    raw_text: str
    question_prompt: str
    answer_prompt: str

@app.post("/api/generate")
async def generate_qna(request: QNARequest):
    logging.info("Received QNA generation request.")
    # Offload the synchronous generation function to a thread so as not to block the event loop.
    final_json = await asyncio.to_thread(
        generate_final_qna_container,
        container_id=request.id,
        raw_text=request.raw_text,
        question_prompt=request.question_prompt,
        answer_prompt=request.answer_prompt,
        num_pairs=20,
        model="gpt-4"
    )
    if not final_json:
        raise HTTPException(status_code=500, detail="Failed to generate Q&A container.")
    
    headers = {"Content-Type": "application/json"}
    try:
        push_response = requests.post(TARGET_API_URL, data=final_json, headers=headers)
        if push_response.status_code != 200:
            logging.error(f"Failed to push Q&A container. Status: {push_response.status_code}, Response: {push_response.text}")
            raise HTTPException(status_code=push_response.status_code, detail="Failed to push Q&A container.")
    except Exception as e:
        logging.error(f"Error pushing Q&A container: {e}")
        raise HTTPException(status_code=500, detail="Error pushing Q&A container.")
    
    return {"detail": "Q&A container generated and pushed successfully.", "qna": json.loads(final_json)}

# ---------------------------
# Server Deployment
# ---------------------------
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
